{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the conda env machine learning to work things\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import scipy.sparse as sparse\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in Insulator Sequences and randomly sampled sequenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Region</th>\n",
       "      <th>Sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Insulator1</td>\n",
       "      <td>CAGGGTTGCCCAGTGCCCCTTGTCACCCCCCGAGCAACACAAGCCA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Insulator2</td>\n",
       "      <td>TTCCTCGTGTGCAGAAATCGGGGGTCAGGGGATGGTTCACATGTGG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Insulator3</td>\n",
       "      <td>AACAGGAAATCCCATTCCCCCATGCCAGTGACTACACCAGGGAAGG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Insulator4</td>\n",
       "      <td>TTAAAATTGTAATGTGTCCTGATTTAGAGGAAACTAATTATTATGT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Insulator5</td>\n",
       "      <td>GTACACACTGGCTCAGTGGATTCATATCCAAAAAGCTGAGCATTGA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Region                                           Sequence\n",
       "0  Insulator1  CAGGGTTGCCCAGTGCCCCTTGTCACCCCCCGAGCAACACAAGCCA...\n",
       "1  Insulator2  TTCCTCGTGTGCAGAAATCGGGGGTCAGGGGATGGTTCACATGTGG...\n",
       "2  Insulator3  AACAGGAAATCCCATTCCCCCATGCCAGTGACTACACCAGGGAAGG...\n",
       "3  Insulator4  TTAAAATTGTAATGTGTCCTGATTTAGAGGAAACTAATTATTATGT...\n",
       "4  Insulator5  GTACACACTGGCTCAGTGGATTCATATCCAAAAAGCTGAGCATTGA..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_sequences = pd.read_csv('1000_8kb_random_samples_upper.tab',sep='\\t',names=['Region','Sequence'])\n",
    "insulator_sequences = pd.read_csv('insulator_library.tab',sep='\\t',names=['Region','Sequence'])\n",
    "combined_sequences=insulator_sequences.append(random_sequences)\n",
    "\n",
    "combined_sequences.head()\n",
    "\n",
    "\n",
    "#random_sample=random_sequences.sample(n=5000,random_state=1)\n",
    "#insulator_sample=insulator_sequences.sample(n=990,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "insulator_dataset = combined_sequences.copy() \n",
    "ins_data = insulator_dataset['Sequence']\n",
    "\n",
    "motif_dict={} \n",
    "\n",
    "for entry in ins_data:\n",
    "    for i in range(len(entry)-5):\n",
    "        sequence = entry[i : i+5]\n",
    "        if sequence not in motif_dict:\n",
    "            motif_dict[sequence]=1\n",
    "        else: \n",
    "            motif_dict[sequence]+=1\n",
    "    \n",
    "motif_list = list(motif_dict)\n",
    "\n",
    "\n",
    "def motif_counter(sequence):\n",
    "    individual_dict = dict.fromkeys(motif_list, 0)\n",
    "    for i in range(len(sequence)-5):\n",
    "        motif = sequence[i : i+5]\n",
    "        individual_dict[motif] += 1\n",
    "    return individual_dict\n",
    "\n",
    "motif_df = pd.DataFrame()\n",
    "\n",
    "for entry in ins_data:\n",
    "    dictionary = motif_counter(entry)\n",
    "    single_motif_df = pd.DataFrame(dictionary, index = [0])\n",
    "    motif_df = motif_df.append(single_motif_df)\n",
    "\n",
    "motif_df.index = range(len(motif_df))\n",
    "insulator_dataset.index = range(len(insulator_dataset))\n",
    "\n",
    "insulator_dataset = insulator_dataset.join(motif_df)\n",
    "motif_df.to_csv('Insulator_motif_dictionary_ext.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 feature(s) (shape=(9352, 0)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-5b06d2bb94e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mimportances\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jamie/miniconda3/envs/bioinfo/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[1;31m# Validate or convert input data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jamie/miniconda3/envs/bioinfo/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    588\u001b[0m                              \u001b[1;34m\" a minimum of %d is required%s.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m                              % (n_features, shape_repr, ensure_min_features,\n\u001b[1;32m--> 590\u001b[1;33m                                 context))\n\u001b[0m\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mwarn_on_dtype\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mdtype_orig\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mdtype_orig\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 feature(s) (shape=(9352, 0)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2350   53]\n",
      " [ 522  193]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(test_y, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "              'max_features': max_features,\n",
    "              'max_depth': max_depth,\n",
    "              'min_samples_split': min_samples_split,\n",
    "              'min_samples_leaf': min_samples_leaf,\n",
    "              'bootstrap': bootstrap}\n",
    "\n",
    "\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation,\n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(train_x, train_y)\n",
    "\n",
    "print(rf_random.best_params_)\n",
    "\n",
    "\n",
    "\n",
    "forest_model = RandomForestClassifier(random_state = 1)\n",
    "forest_model.fit(train_x, train_y)\n",
    "preds = forest_model.predict(test_x)\n",
    "print(mean_absolute_error(test_y, preds))\n",
    "\n",
    "#Permutation importance\n",
    "#perm = PermutationImportance(forest_model, random_state = 1).fit(test_x, test_y)\n",
    "#eli5.show_weights(perm,feature_names = test_x.columns.tolist())\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "XGB_model = XGBRegressor(n_estimators = 1000, learning_rate = 0.05)\n",
    "XGB_model.fit(train_x, train_y, early_stopping_rounds=5, eval_set=[(test_x, test_y)], verbose = False)\n",
    "\n",
    "XGB_preds = XGB_model.predict(test_x)\n",
    "print(mean_absolute_error(test_y, XGB_preds))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#other stuff I tried:\n",
    "\n",
    "#new_splitprime = ANN_dataset['seq_3utr'].str.split(\"\", expand = True) \n",
    "#ANN_dataset = ANN_dataset.drop(['seq_3utr', 'index'], axis = 1)\n",
    "#ANN_dataset = ANN_dataset.join(new_splitprime)\n",
    "\n",
    "#from numpy import array\n",
    "#from numpy import argmax\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "#from sklearn.preprocessing import OneHotEncoder\n",
    "#import re\n",
    "\n",
    "\n",
    "#onehot = []\n",
    "\n",
    "#for index, row in ANN_dataset.iterrows():\n",
    "#    seq_array = array(list(row['seq_3utr']))\n",
    "#    label_encoder = LabelEncoder()\n",
    "#    integer_encoded_seq = label_encoder.fit_transform(seq_array)\n",
    "#    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "#    integer_encoded_seq = integer_encoded_seq.reshape(len(integer_encoded_seq), 1)\n",
    "#    onehot_encoded_seq = onehot_encoder.fit_transform(integer_encoded_seq)\n",
    "#    onehot.append(onehot_encoded_seq)\n",
    "\n",
    "#ANN_dataset['onehotencoded'] = onehot\n",
    "\n",
    "#ANN_y = ANN_dataset['Halflife']\n",
    "#ANN_x = ANN_dataset['onehotencoded']\n",
    "\n",
    "#for row in ANN_x:\n",
    "#    np.split(row, 4)\n",
    "\n",
    "#for row in ANN_x:\n",
    "#    for i in ANN_x:\n",
    "#        RNA_seq = ANN_x[i : i+5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction=clf.predict(test_x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(mean_absolute_error(test_y, preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
