{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Created on Wed Nov  6 14:52:11 2019\n",
    "@author: Jamie Billington\n",
    "\"\"\"\n",
    "Notebook outlining an approach to build a classifier using keras neural networks to discriminate between \n",
    "insulators at chromatin transition regions and non insulating sequences. Prospective insulators taken from 'Genome-wide prediction and analysis of human chromatin boundary elements' Wang et al. 2012 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "## Initialise\n",
    "##############################################################################\n",
    "# Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import random\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.callbacks import CSVLogger\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import scipy.sparse as sparse\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we read in the DNA sequences for our regions of interest- chromatin transition regions (CTR) and either a)random sequences or b) sequences flanking either side of the CTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Sequence\n",
      "0  TTATCTGAAAAATCACAGCTTGGGTGCATGTGAAGCCAGAGGAGCA...\n",
      "1  AGGGCCAGTGTCAGAGCTCCCCTGTGCCCCTGTCCCGCCACAGCCA...\n",
      "2  GTTAACTAAAAAATGCAAAAAAATTAACAGCAGATTAAAGTCAGTG...\n",
      "3  GAGATGCTGGTACACAGTTACTAGGGTATCTTCCCAAAATAAAACC...\n",
      "4  GGGGGAAGAAGGTAGATGAGAACTCTGCAGTTTCTGCTTAGTTTTT...\n"
     ]
    }
   ],
   "source": [
    "os.chdir('/Users/jamie/Documents/Coding/Machine Learning/')\n",
    "\n",
    "#Import 8kb regions generated in R which contain either a random sample of the genome or a chromatin transition region\n",
    "random_8kb_sequences = pd.read_csv('1000_8kb_random_samples_upper.tab',sep='\\t',names=['Region','Sequence'])\n",
    "sequence_extract=pd.Series(random_8kb_sequences['Sequence'])\n",
    "sliced=sequence_extract.str.slice(start=3500,stop=4500)\n",
    "random_sequences=pd.DataFrame(sliced)\n",
    "random_sequences=random_sequences[0:2761]\n",
    "\n",
    "# Option to Slice out the 1000bp predicted inuslator chunk at the core of each Chromatin transition region\n",
    "ctr_sequences = pd.read_csv('insulator_library.tab',sep='\\t',names=['Region','Sequence'])\n",
    "sequence_extract=pd.Series(ctr_sequences['Sequence'])\n",
    "sliced=sequence_extract.str.slice(start=3500,stop=4500)\n",
    "insulator_sequences=pd.DataFrame(sliced)\n",
    "\n",
    "\n",
    "\n",
    "combined_sequences=insulator_sequences.append(random_sequences)\n",
    "print(combined_sequences.head())\n",
    "\n",
    "insulator_dataset= combined_sequences.copy()\n",
    "## Running optimised model on whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next one hot encode each of the insulator and control sequences in the library to generate a 4 x Number of base pairs array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that this works sequence entry 2 base 2 should be G [0,0,1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "# One hot encoder\n",
    "\n",
    "def one_hot_encode(df, col, seq_len):\n",
    "    # Dictionary returning one-hot encoding of nucleotides. \n",
    "    nuc_d = {'a':[1,0,0,0],'c':[0,1,0,0],'g':[0,0,1,0],'t':[0,0,0,1], 'n':[0,0,0,0]}\n",
    "    \n",
    "    # Creat empty matrix.\n",
    "    vectors=np.empty([len(df),seq_len,4])\n",
    "    \n",
    "    # Iterate through sequences and one-hot encode\n",
    "    for i,seq in enumerate(df[col].str[:seq_len]): \n",
    "        seq = seq.lower()\n",
    "        a = np.array([nuc_d[x] for x in seq])\n",
    "        vectors[i] = a\n",
    "    return vectors\n",
    "\n",
    "one_hot_encoded=one_hot_encode(insulator_dataset,col='Sequence',seq_len=999)\n",
    "print(one_hot_encoded[1][1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next comes labeling of the data for the basis of classification. Insulators are labelled as 1 (TRUE), non insulators assisgned 0 (FALSE)\n",
    "Array of scores are encoded in y and the one hot coded array is relablled x for convential layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##############################################################################\n",
    "# Generating the output labels\n",
    "\n",
    "ins=[1]*len(insulator_sequences)\n",
    "data=pd.DataFrame(ins)\n",
    "rando=[0]*len(random_sequences)\n",
    "scores=data.append(rando)\n",
    "y = np.ravel(scores)\n",
    "\n",
    "########################\n",
    "\n",
    "x=one_hot_encoded\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "y_binary = to_categorical(y)\n",
    "\n",
    "\n",
    "train_x,test_x,train_y,test_y=train_test_split(x,y, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1.]],\n",
       "\n",
       "       [[1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.]],\n",
       "\n",
       "       [[0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        ...,\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        ...,\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0.]],\n",
       "\n",
       "       [[1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1.]],\n",
       "\n",
       "       [[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for building neural network model. Only a preliminary run to check the code works- no hyperparameter optimisation here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4141/4141 [==============================] - 244s 59ms/step - loss: 0.7081 - acc: 0.4943\n",
      "Epoch 2/10\n",
      "4141/4141 [==============================] - 257s 62ms/step - loss: 0.6935 - acc: 0.4907\n",
      "Epoch 3/10\n",
      "4141/4141 [==============================] - 225s 54ms/step - loss: 0.6932 - acc: 0.4832\n",
      "Epoch 4/10\n",
      "4141/4141 [==============================] - 226s 54ms/step - loss: 0.6933 - acc: 0.4979\n",
      "Epoch 5/10\n",
      "4141/4141 [==============================] - 630s 152ms/step - loss: 0.6933 - acc: 0.4938\n",
      "Epoch 6/10\n",
      "4141/4141 [==============================] - 214s 52ms/step - loss: 0.6933 - acc: 0.4890\n",
      "Epoch 7/10\n",
      "4141/4141 [==============================] - 219s 53ms/step - loss: 0.6933 - acc: 0.4977\n",
      "Epoch 8/10\n",
      "4141/4141 [==============================] - 221s 53ms/step - loss: 0.6933 - acc: 0.4977\n",
      "Epoch 9/10\n",
      "4141/4141 [==============================] - 516s 125ms/step - loss: 0.6933 - acc: 0.4943\n",
      "Epoch 10/10\n",
      "4141/4141 [==============================] - 219s 53ms/step - loss: 0.6933 - acc: 0.4907\n"
     ]
    }
   ],
   "source": [
    "\n",
    "################################################################################################################\n",
    "\n",
    "def train_model(x, y, nb_epoch, conv_layers=2, inp_len=999, border_mode='same', nbr_filters=128, filter_len=8, \n",
    "                conv_dropout2=0.4, conv_dropout3=0, dense_layers=1, nodes1=100, nodes2=40, nodes3=70, dense_dropout1=0, \n",
    "                dense_dropout2=0, dense_dropout3=0, loss='binary_crossentropy'):\n",
    "     \n",
    "    model = Sequential()\n",
    "    \n",
    "    # Convolutional layers\n",
    "    if conv_layers >= 1:\n",
    "        model.add(Conv1D(activation=\"relu\", input_shape=(inp_len, 4), padding=border_mode, filters=nbr_filters, \n",
    "                         kernel_size=filter_len))\n",
    "        \n",
    "    if conv_layers >= 2:\n",
    "        model.add(Conv1D(activation=\"relu\", input_shape=(inp_len, 1), padding=border_mode, filters=nbr_filters, \n",
    "                         kernel_size=filter_len))\n",
    "        model.add(Dropout(conv_dropout2))\n",
    "        \n",
    "    if conv_layers >= 3:\n",
    "        model.add(Conv1D(activation=\"relu\", input_shape=(inp_len, 1), padding=border_mode, filters=nbr_filters, \n",
    "                         kernel_size=filter_len))\n",
    "        model.add(Dropout(conv_dropout3))\n",
    "    \n",
    "    # Flatten\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Dense layers\n",
    "    if dense_layers >= 1:\n",
    "        model.add(Dense(nodes1, activation='relu'))\n",
    "        model.add(Dropout(dense_dropout1))\n",
    "        \n",
    "    if dense_layers >= 2:\n",
    "        model.add(Dense(nodes2, activation='relu'))\n",
    "        model.add(Dropout(dense_dropout2))\n",
    "        \n",
    "    if dense_layers >= 3:\n",
    "        model.add(Dense(nodes3, activation='relu'))\n",
    "        model.add(Dropout(dense_dropout3))\n",
    "        \n",
    "    # Output layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile the model\n",
    "    adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    model.compile(loss=loss, optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(x, y, batch_size=10, epochs=nb_epoch, verbose=1)\n",
    "    return model\n",
    "\n",
    "# Run model\n",
    "model = train_model(train_x,train_y, nb_epoch=10)\n",
    "predictions = model.predict(test_x)\n",
    "######################################################################\n",
    "#model.save('insulator_training_parameters.h5')\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the models precitions: \n",
    "Print confusion matrix for IDing false + and - (In basic state model does no better than random )\n",
    "Pretty sure that given this data will get a lot of overfitting -samples number ~ 5000 length of sample 1000bp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-673ef63c387a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#############################################################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredict_bin\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcm_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_bin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "#############################################################################################################################\n",
    "predict_bin= np.where(predictions>=0.5, 1, 0)\n",
    "\n",
    "# Analysis\n",
    "cm_all = confusion_matrix(test_y, predict_bin)\n",
    "print(cm_all)\n",
    "\n",
    "\n",
    "test_acc = (cm_all[0,0]+cm_all[1,1])/np.sum(cm_all) # accuracy achieved on test data; number predicted correctly / total\n",
    "pred_one_acc = cm_all[1,1]/(cm_all[0,1]+cm_all[1,1]) # proportion of ones predicted correctly on test data\n",
    "pred_zero_acc = cm_all[0,0]/(cm_all[0,0]+cm_all[1,0]) # prcmoportion of zeros predicted correctly on test data\n",
    "f0_5 = fbeta_score(predict_bin,test_y, beta=0.5)  # punishes false positives more (i.e. promoters that are predicted as being stress activated but aren't)\n",
    "\n",
    "\n",
    "print(\"Test accuracy: \" + str(round(test_acc,4)))\n",
    "print(\"Ones accuracy: \" + str(round(pred_one_acc,4)))  \n",
    "print(\"Zeros accuracy: \" + str(round(pred_zero_acc,4)))  \n",
    "print(\"F0.5 score: \" + str(round(f0_5,4)))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
